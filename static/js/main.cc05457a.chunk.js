(this["webpackJsonphamza-web"]=this["webpackJsonphamza-web"]||[]).push([[0],{100:function(e,t){},109:function(e,t,a){"use strict";a.r(t);a(0);var s=a(24),i=a.n(s),c=(a(56),a(8)),n=a(20),r=a(18),o=a(12),l=a(21),d=a(16),h=a(43),m=(a(26),a(27),a(1));var j=function(){return Object(m.jsxs)(m.Fragment,{children:[Object(m.jsx)(d.a,{className:"mx-auto",variant:"dark",sticky:"top",style:{backgroundColor:"#c3aeff"},children:Object(m.jsxs)(l.a,{children:[Object(m.jsx)(c.b,{className:"nav-link page-logo active",to:"/"}),Object(m.jsx)(c.b,{className:"nav-link active",to:"/",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"Home"})}),Object(m.jsx)(c.b,{className:"nav-link",to:"/publications",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"Publications"})}),Object(m.jsx)(c.b,{className:"nav-link",to:"/cv",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"CV"})})]})}),Object(m.jsx)("div",{className:"App",children:Object(m.jsx)(n.a,{fluid:!0,className:"mx-auto",style:{margin:"1rem 3rem 0 3rem"},children:Object(m.jsxs)(r.a,{children:[Object(m.jsxs)(o.a,{xs:12,sm:12,md:6,lg:4,xl:4,className:"text-center mx-auto",children:[Object(m.jsx)("div",{className:"cropped mx-auto",children:Object(m.jsx)("img",{src:"/IMG_7839_Original.jpg",className:"profile",alt:"Hamza Elhamdadi"})}),Object(m.jsx)("div",{className:"bckgrnd cardbodytext mx-auto",children:Object(m.jsx)("h1",{children:"Hamza Elhamdadi"})})]}),Object(m.jsxs)(o.a,{xs:12,sm:12,md:6,lg:5,xl:4,className:"mx-auto",children:[Object(m.jsx)(r.a,{style:{marginTop:"10px",color:"#7050a0"},children:Object(m.jsxs)(o.a,{lg:12,className:"text-left mx-auto",children:[Object(m.jsx)("h2",{children:"About Me"}),Object(m.jsxs)("p",{style:{marginBottom:10},children:["I am a doctoral student in the ",Object(m.jsx)("a",{style:{margin:0},target:"_",href:"https://groups.cs.umass.edu/hci-vis/",children:" HCI-Vis Lab"}),"\xa0 at the ",Object(m.jsx)("a",{style:{margin:0},target:"_",href:"https://www.umass.edu",children:" University of Massachusetts Amherst"}),". I joined UMass in the Fall of 2021. I received my Master's and Bachelor's degrees from the Computer Science and Engineering at the ",Object(m.jsx)("a",{style:{margin:0},target:"_",href:"https://usf.edu",children:"University of South Florida"}),"."]}),Object(m.jsxs)("p",{style:{marginBottom:"10%"},children:["I am currently pursuing a Ph.D. under the advising of ",Object(m.jsx)("a",{style:{margin:0},target:"_",href:"https://cyxiong.com",children:"Dr. Cindy Xiong"}),". My research interests in the area of Data & Information Visualization include Perception in Visualization and Topological Data Analysis. My recent published work applies Topological Data Analysis to Emotion Detection in the field of Affective Computing."]})]})}),Object(m.jsx)(r.a,{className:"text-left mx-auto",style:{color:"#7050a0"},children:Object(m.jsxs)(o.a,{className:"bckgrnd cardbodytext",style:{marginBottom:"10%"},xs:12,sm:12,md:12,lg:12,xl:12,children:[Object(m.jsx)("h4",{children:"Recent News"}),Object(m.jsxs)("ul",{className:"table-list",children:[Object(m.jsx)("li",{className:"table-element",children:Object(m.jsx)("p",{children:"Paper accepted to IEEE Vis 2021 and TVCG"})}),Object(m.jsx)("li",{className:"table-element element-right",children:Object(m.jsx)("b",{children:"Fall 2021"})})]}),Object(m.jsx)("ul",{className:"table-list",children:Object(m.jsx)("li",{className:"table-element",children:Object(m.jsx)("a",{href:"https://www.youtube.com/watch?v=buqJx0w5PiQ",target:"_",style:{textAlign:"right"},children:"Watch my IEEE Vis Presentation here!"})})}),Object(m.jsxs)("ul",{className:"table-list",children:[Object(m.jsx)("li",{className:"table-element",children:Object(m.jsx)("p",{children:"Accepted to Ph.D. program at UMass Amherst"})}),Object(m.jsx)("li",{className:"table-element element-right",children:Object(m.jsx)("b",{children:"Fall 2021"})})]})]})})]}),Object(m.jsxs)(o.a,{xs:12,sm:12,md:12,lg:3,xl:4,children:[Object(m.jsx)(h.b,{dataSource:{sourceType:"profile",screenName:"ElhamdadiHamza"},options:{height:"600vh",chrome:"noheader, nofooter"}}),Object(m.jsx)(h.a,{username:"ElhamdadiHamza",options:{size:"small"}})]})]})})})]})},u=a(9);var p=function(){return Object(m.jsxs)(m.Fragment,{children:[Object(m.jsx)(d.a,{className:"mx-auto",variant:"dark",sticky:"top",style:{backgroundColor:"#c3aeff"},children:Object(m.jsxs)(l.a,{children:[Object(m.jsx)(c.b,{className:"nav-link page-logo active",to:"/"}),Object(m.jsx)(c.b,{className:"nav-link active",to:"/",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"Home"})}),Object(m.jsx)(c.b,{className:"nav-link",to:"/publications",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"Publications"})}),Object(m.jsx)(c.b,{className:"nav-link",to:"/cv",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"CV"})})]})}),Object(m.jsx)("div",{className:"App",children:Object(m.jsx)(n.a,{className:"mx-auto",style:{color:"#7050a0"},children:Object(m.jsx)(r.a,{children:Object(m.jsxs)(o.a,{xs:12,lg:8,className:"mx-auto",children:[Object(m.jsxs)(u.a,{className:"project mx-auto",children:[Object(m.jsx)(u.a.Header,{className:"bckgrnd cardbodytext hdr",children:"How Do We Measure Trust in Visual Data Communication?"}),Object(m.jsx)(u.a.Img,{src:"/measuring_trust_teaser.png",className:"teaser",alt:"Teaser for trust project"}),Object(m.jsxs)(u.a.Body,{className:"bckgrnd cardbodytext text-left",children:[Object(m.jsx)("p",{children:Object(m.jsx)("b",{children:"Hamza Elhamdadi, Aimen Gaba, Yea-Seul Kim, Cindy Xiong"})}),Object(m.jsx)("h6",{children:"Abstract:"}),Object(m.jsx)("p",{children:"Trust is fundamental to effective visual data communication between the visualization designer and the reader. Although personal experience and preference influence readers\u2019 trust in visualizations, visualization designers can leverage design techniques to create visualizations that evoke a \u201ccalibrated trust,\u201d at which readers arrive after critically evaluating the information presented. To systematically un- derstand what drives readers to engage in \u201ccalibrated trust,\u201d we must first equip ourselves with reliable and valid methods for measuring trust. Computer science and data visualization researchers have not yet reached a consensus on a trust definition or metric, which are essential to building a comprehensive trust model in human- data interaction. On the other hand, social scientists and behavioral economists have developed and perfected metrics that can measure generalized and interpersonal trust, which the visualization community can reference, modify, and adapt for our needs. In this paper, we gather existing methods for evaluating trust from other disciplines and discuss how we might use them to measure, define, and model trust in data visualization research. Specifically, we discuss quan- titative surveys from social sciences, trust games from behavioral economics, measuring trust through measuring belief updating, and measuring trust through perceptual methods. We assess the potential issues with these methods and consider how we can systematically apply them to visualization research."}),Object(m.jsx)("h6",{children:"Venue: IEEE BELIV, 2022"}),Object(m.jsx)("a",{href:"/trust_measure.pdf",target:"_",className:"bckgrnd stretched-link"}),Object(m.jsx)("p",{className:"tags workshop-tag"})]})]}),Object(m.jsxs)(u.a,{className:"project mx-auto",children:[Object(m.jsx)(u.a.Header,{className:"bckgrnd cardbodytext hdr",children:"Using Processing Fluency as a Metric of Trust in Scatterplot Visualizations"}),Object(m.jsx)(u.a.Img,{src:"/tweet_visual_content.png",className:"teaser",alt:"Teaser for trust project"}),Object(m.jsxs)(u.a.Body,{className:"bckgrnd cardbodytext text-left",children:[Object(m.jsx)("p",{children:Object(m.jsx)("b",{children:"Hamza Elhamdadi, Lace Padilla, Cindy Xiong"})}),Object(m.jsx)("h6",{children:"Abstract:"}),Object(m.jsx)("p",{children:"Establishing trust with readers is an important first step in visual data communication. But what makes a visualization trustworthy? Psychology and behavioral economics research has found processing fluency (i.e., speed and accuracy of perceiving and processing a stimulus) is central to perceived trust. We examine the association between processing fluency and trust in visualizations through two empirical studies. In Experiment 1, we tested the effect of camouflaging a visualization on processing fluency. Participants estimated the proportion of data values within a specified range for six camouflaged visualizations and one non-camouflaged control; they also reported their perceived difficulty for each of the visualizations. Camouflaged visualizations produced less accurate estimations compared to the control. In Experiment 2, we created a decision task based on trust games adapted from behavioral economics. We asked participants to invest money in two hypothetical companies and report how much they trust each company. One company communicates its strategy with a camouflaged visualization, the other with a controlled visualization. Participants tended to invest less money in the company presenting a camouflaged visualization. Hence, we found support for the hypothesis that processing fluency is key to the perception of trust in visual data communication."}),Object(m.jsx)("h6",{children:"Venue: IEEE TREX, 2022"}),Object(m.jsx)("a",{href:"/pfluency.pdf",target:"_",className:"bckgrnd stretched-link"}),Object(m.jsx)("p",{className:"tags workshop-tag"})]})]}),Object(m.jsxs)(u.a,{className:"project mx-auto",children:[Object(m.jsx)(u.a.Header,{className:"bckgrnd cardbodytext hdr",children:"Processing Fluency Improves Trust In Scatterplot Visualizations"}),Object(m.jsx)(u.a.Img,{src:"/poster_teaser.png",className:"teaser",alt:"Teaser for trust project"}),Object(m.jsxs)(u.a.Body,{className:"bckgrnd cardbodytext text-left",children:[Object(m.jsx)("p",{children:Object(m.jsx)("b",{children:"Hamza Elhamdadi, Lace Padilla, Cindy Xiong"})}),Object(m.jsx)("h6",{children:"Abstract:"}),Object(m.jsx)("p",{children:"Trust plays a significant role in how people perceive, discount, or dismiss scientific information and make critical decisions. Therefore, establishing trust is a critical first step in visual data communication. But what makes a visualization trustworthy? Researchers in psychology and behavioral economics have identified processing fluency (i.e., speed and accuracy of perceiving and processing a stimulus) as a key factor impacting trust perception. We examine the association between processing fluency and trust in visualizations through two empirical studies. We manipulated fluency by creating camouflaged visualizations that are more difficult to process. In Experiment 1, we tested the effect of camouflaging a visualization on processing fluency. Participants completed a perceptual task with six camouflaged visualizations and one non-camouflaged control. The task involved estimating the proportion of data values within a range and reporting the difficulty of doing so. We found the camouflaged visualizations produced less accurate estimations compared to the control. In Experiment 2, we created a decision task based on trust games adapted from behavioral economics. We asked participants to invest money in two hypothetical companies and report how much they trust each company. One company communicates its strategy with a camouflaged visualization, the other with a controlled visualization. The results revealed that participants tend to invest less money in the company presenting a camouflaged visualization. We found that processing fluency is a key factor in the perception of trust in visual data communication."}),Object(m.jsx)("h6",{children:"Venue: IEEE VIS Posters, 2022"}),Object(m.jsx)("a",{href:"/visposter2022.pdf",target:"_",className:"bckgrnd stretched-link"}),Object(m.jsx)("p",{className:"tags poster-tag"})]})]}),Object(m.jsxs)(u.a,{className:"project mx-auto",children:[Object(m.jsx)(u.a.Header,{className:"bckgrnd cardbodytext hdr",children:"AffectiveTDA: Using Topological Data Analysis to Improve Analysis and Explainability in Affective Computing"}),Object(m.jsx)(u.a.Img,{src:"/affectiveTDA_teaser.png",className:"teaser",alt:"Teaser for AffectiveTDA project"}),Object(m.jsxs)(u.a.Body,{className:"bckgrnd cardbodytext text-left",children:[Object(m.jsx)("p",{children:Object(m.jsx)("b",{children:"Hamza Elhamdadi, Shaun Canavan, Paul Rosen"})}),Object(m.jsx)("h6",{children:"Abstract:"}),Object(m.jsx)("p",{children:"We present an approach utilizing Topological Data Analysis to study the structure of face poses used in affective computing, i.e., the process of recognizing human emotion. The approach uses a conditional comparison of different emotions, both respective and irrespective of time, with multiple topological distance metrics, dimension reduction techniques, and face subsections (e.g., eyes, nose, mouth, etc.). The results confirm that our topology-based approach captures known patterns, distinctions between emotions, and distinctions between individuals, which is an important step towards more robust and explainable emotion recognition by machines."}),Object(m.jsx)("h6",{children:"Venue: IEEE VIS, 2021"}),Object(m.jsx)("a",{href:"https://arxiv.org/pdf/2107.08573.pdf",target:"_",className:"bckgrnd stretched-link"}),Object(m.jsx)("p",{className:"tags conference-tag"})]})]}),Object(m.jsxs)(u.a,{className:"project mx-auto",children:[Object(m.jsx)(u.a.Header,{className:"bckgrnd cardbodytext hdr",children:"Recognizing Emotion in the Wild using Multimodal Data"}),Object(m.jsxs)(u.a.Body,{className:"bckgrnd cardbodytext text-left",children:[Object(m.jsx)("p",{children:Object(m.jsx)("b",{children:"Shivam Srivastava, Saandeep Aathreya, Saurabh Hinduja, Sk Rahatul Jannat, Hamza Elhamdadi, and Shaun Canavan"})}),Object(m.jsx)("h6",{children:"Abstract:"}),Object(m.jsx)("p",{children:"In this work, we present our approach for all four tracks of the eighth Emotion Recognition in the Wild Challenge (EmotiW 2020). The four tasks are group emotion recognition, driver gaze prediction, predicting engagement in the wild, and emotion recognition using physiological signals. We explore multiple approaches including classical machine learning tools such as random forests, state of the art deep neural networks, and multiple fusion and ensemblebased approaches. We also show that similar approaches can be used across tracks as many of the features generalize well to the different problems (e.g. facial features). We detail evaluation results that are either comparable to or outperform the baseline results for both the validation and testing for most of the tracks."}),Object(m.jsx)("h6",{children:"Venue: ICMI 2020"}),Object(m.jsx)("a",{href:"https://dl.acm.org/doi/pdf/10.1145/3382507.3417970",target:"_",className:"bckgrnd stretched-link"}),Object(m.jsx)("p",{className:"tags conference-tag"})]})]})]})})})})]})},b=a(34);a(101);var g=function(){return Object(m.jsxs)(m.Fragment,{children:[Object(m.jsx)(d.a,{className:"mx-auto",variant:"dark",sticky:"top",style:{backgroundColor:"#c3aeff"},children:Object(m.jsxs)(l.a,{children:[Object(m.jsx)(c.b,{className:"nav-link page-logo active",to:"/"}),Object(m.jsx)(c.b,{className:"nav-link active",to:"/",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"Home"})}),Object(m.jsx)(c.b,{className:"nav-link",to:"/publications",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"Publications"})}),Object(m.jsx)(c.b,{className:"nav-link",to:"/cv",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"CV"})})]})}),Object(m.jsx)("div",{className:"App",children:Object(m.jsxs)(n.a,{className:"mx-auto",children:[Object(m.jsx)(b.Worker,{workerUrl:"https://unpkg.com/pdfjs-dist@2.6.347/build/pdf.worker.min.js"}),Object(m.jsx)(o.a,{xs:12,lg:8,className:"mx-auto",children:Object(m.jsx)(b.Viewer,{fileUrl:"/CV-Elhamdadi-2022-Sep-17.pdf",defaultScale:b.SpecialZoomLevel.PageWidth})})]})})]})};a(102);var x=function(){return Object(m.jsxs)(m.Fragment,{children:[Object(m.jsxs)(d.a,{bg:"dark",variant:"dark",children:[Object(m.jsx)(d.a.Brand,{style:{marginLeft:"10%"},href:"#home",children:"Hamza Elhamdadi"}),Object(m.jsxs)(l.a,{className:"mr-auto",children:[Object(m.jsx)(c.b,{class:"nav-link",to:"/",children:"Home"}),Object(m.jsx)(c.b,{class:"nav-link",to:"/Publications",children:"Publications"}),Object(m.jsx)(c.b,{class:"nav-link active",to:"/art",children:"My Art"})]})]}),Object(m.jsx)("div",{className:"App",children:Object(m.jsx)("header",{className:"App-header",children:Object(m.jsx)(n.a,{children:Object(m.jsxs)(r.a,{children:[Object(m.jsx)(o.a,{lg:2}),Object(m.jsx)(o.a,{xs:12,lg:10,children:Object(m.jsx)(r.a,{id:"grid"})})]})})})}),Object(m.jsx)("script",{children:"$('.grid').isotope(specs)"})]})},f=function(e){e&&e instanceof Function&&a.e(3).then(a.bind(null,111)).then((function(t){var a=t.getCLS,s=t.getFID,i=t.getFCP,c=t.getLCP,n=t.getTTFB;a(e),s(e),i(e),c(e),n(e)}))},O=a(7),v=document.getElementById("root");i.a.render(Object(m.jsx)(c.a,{children:Object(m.jsxs)(O.c,{children:[Object(m.jsx)(O.a,{exact:!0,path:"/",component:j}),Object(m.jsx)(O.a,{path:"/publications",component:p}),Object(m.jsx)(O.a,{path:"/art",component:x}),Object(m.jsx)(O.a,{path:"/cv",component:g})]})}),v),f()},27:function(e,t,a){},47:function(e,t){},56:function(e,t,a){},96:function(e,t){},97:function(e,t){},98:function(e,t){},99:function(e,t){}},[[109,1,2]]]);
//# sourceMappingURL=main.cc05457a.chunk.js.map