(this["webpackJsonphamza-web"]=this["webpackJsonphamza-web"]||[]).push([[0],{100:function(e,t){},109:function(e,t,a){"use strict";a.r(t);a(0);var s=a(24),c=a.n(s),n=(a(56),a(8)),i=a(20),l=a(17),r=a(11),o=a(21),d=a(15),j=a(43),h=(a(26),a(27),a(1));var m=function(){return Object(h.jsxs)(h.Fragment,{children:[Object(h.jsx)(d.a,{className:"mx-auto",variant:"dark",sticky:"top",style:{backgroundColor:"#c3aeff"},children:Object(h.jsxs)(o.a,{children:[Object(h.jsx)(n.b,{className:"nav-link page-logo active",to:"/"}),Object(h.jsx)(n.b,{className:"nav-link active",to:"/",children:Object(h.jsx)("h5",{style:{marginTop:10},children:"Home"})}),Object(h.jsx)(n.b,{className:"nav-link",to:"/publications",children:Object(h.jsx)("h5",{style:{marginTop:10},children:"Publications"})}),Object(h.jsx)(n.b,{className:"nav-link",to:"/cv",children:Object(h.jsx)("h5",{style:{marginTop:10},children:"CV"})})]})}),Object(h.jsx)("div",{className:"App",children:Object(h.jsx)(i.a,{fluid:!0,className:"mx-auto",style:{margin:"1rem 3rem 0 3rem"},children:Object(h.jsxs)(l.a,{children:[Object(h.jsxs)(r.a,{xs:12,sm:12,md:6,lg:4,xl:4,className:"text-center mx-auto",children:[Object(h.jsx)("div",{className:"cropped mx-auto",children:Object(h.jsx)("img",{src:"/IMG_7839_Original.jpg",className:"profile",alt:"Hamza Elhamdadi"})}),Object(h.jsx)("div",{className:"bckgrnd cardbodytext mx-auto",children:Object(h.jsx)("h1",{children:"Hamza Elhamdadi"})})]}),Object(h.jsxs)(r.a,{xs:12,sm:12,md:6,lg:5,xl:4,className:"mx-auto",children:[Object(h.jsx)(l.a,{style:{marginTop:"10px",color:"#7050a0"},children:Object(h.jsxs)(r.a,{lg:12,className:"text-left mx-auto",children:[Object(h.jsx)("h2",{children:"About Me"}),Object(h.jsxs)("p",{style:{marginBottom:10},children:["I am a doctoral student in the ",Object(h.jsx)("a",{style:{margin:0},target:"_",href:"https://groups.cs.umass.edu/hci-vis/",children:" HCI-Vis Lab"}),"\xa0 at the ",Object(h.jsx)("a",{style:{margin:0},target:"_",href:"https://www.umass.edu",children:" University of Massachusetts Amherst"}),". I joined UMass in the Fall of 2021. I received my Master's and Bachelor's degrees from the Computer Science and Engineering at the ",Object(h.jsx)("a",{style:{margin:0},target:"_",href:"https://usf.edu",children:"University of South Florida"}),"."]}),Object(h.jsxs)("p",{style:{marginBottom:"10%"},children:["I am currently pursuing a Ph.D. under the advising of ",Object(h.jsx)("a",{style:{margin:0},target:"_",href:"https://cyxiong.com",children:"Dr. Cindy Xiong"}),". My research interests in the area of Data & Information Visualization include Perception in Visualization and Topological Data Analysis. My recent published work applies Topological Data Analysis to Emotion Detection in the field of Affective Computing."]})]})}),Object(h.jsx)(l.a,{className:"text-left mx-auto",style:{color:"#7050a0"},children:Object(h.jsxs)(r.a,{className:"bckgrnd cardbodytext",style:{marginBottom:"10%"},xs:12,sm:12,md:12,lg:12,xl:12,children:[Object(h.jsx)("h4",{children:"Recent News"}),Object(h.jsxs)("ul",{className:"table-list",children:[Object(h.jsx)("li",{className:"table-element",children:Object(h.jsx)("p",{children:"Paper accepted to IEEE Vis 2021 and TVCG"})}),Object(h.jsx)("li",{className:"table-element element-right",children:Object(h.jsx)("b",{children:"Fall 2021"})})]}),Object(h.jsx)("ul",{className:"table-list",children:Object(h.jsx)("li",{className:"table-element",children:Object(h.jsx)("a",{href:"https://www.youtube.com/watch?v=buqJx0w5PiQ",target:"_",style:{textAlign:"right"},children:"Watch my IEEE Vis Presentation here!"})})}),Object(h.jsxs)("ul",{className:"table-list",children:[Object(h.jsx)("li",{className:"table-element",children:Object(h.jsx)("p",{children:"Accepted to Ph.D. program at UMass Amherst"})}),Object(h.jsx)("li",{className:"table-element element-right",children:Object(h.jsx)("b",{children:"Fall 2021"})})]})]})})]}),Object(h.jsxs)(r.a,{xs:12,sm:12,md:12,lg:3,xl:4,children:[Object(h.jsx)(j.b,{dataSource:{sourceType:"profile",screenName:"ElhamdadiHamza"},options:{height:"80vh",chrome:"noheader, nofooter"}}),Object(h.jsx)(j.a,{username:"ElhamdadiHamza",options:{size:"small"}})]})]})})})]})},b=a(18);var x=function(){return Object(h.jsxs)(h.Fragment,{children:[Object(h.jsx)(d.a,{className:"mx-auto",variant:"dark",sticky:"top",style:{backgroundColor:"#c3aeff"},children:Object(h.jsxs)(o.a,{children:[Object(h.jsx)(n.b,{className:"nav-link page-logo active",to:"/"}),Object(h.jsx)(n.b,{className:"nav-link active",to:"/",children:Object(h.jsx)("h5",{style:{marginTop:10},children:"Home"})}),Object(h.jsx)(n.b,{className:"nav-link",to:"/publications",children:Object(h.jsx)("h5",{style:{marginTop:10},children:"Publications"})}),Object(h.jsx)(n.b,{className:"nav-link",to:"/cv",children:Object(h.jsx)("h5",{style:{marginTop:10},children:"CV"})})]})}),Object(h.jsx)("div",{className:"App",children:Object(h.jsx)(i.a,{className:"mx-auto",style:{color:"#7050a0"},children:Object(h.jsx)(l.a,{children:Object(h.jsxs)(r.a,{xs:12,lg:8,className:"mx-auto",children:[Object(h.jsxs)(b.a,{className:"project mx-auto",children:[Object(h.jsx)(b.a.Header,{className:"bckgrnd cardbodytext",children:"AffectiveTDA: Using Topological Data Analysis to Improve Analysis and Explainability in Affective Computing"}),Object(h.jsx)(b.a.Img,{src:"/affectiveTDA_teaser.png",className:"teaser",alt:"Teaser for AffectiveTDA project"}),Object(h.jsxs)(b.a.Body,{className:"bckgrnd cardbodytext text-left",children:[Object(h.jsx)("p",{children:Object(h.jsx)("b",{children:"Hamza Elhamdadi, Shaun Canavan, Paul Rosen"})}),Object(h.jsx)("h6",{children:"Abstract:"}),Object(h.jsx)("p",{children:"We present an approach utilizing Topological Data Analysis to study the structure of face poses used in affective computing, i.e., the process of recognizing human emotion. The approach uses a conditional comparison of different emotions, both respective and irrespective of time, with multiple topological distance metrics, dimension reduction techniques, and face subsections (e.g., eyes, nose, mouth, etc.). The results confirm that our topology-based approach captures known patterns, distinctions between emotions, and distinctions between individuals, which is an important step towards more robust and explainable emotion recognition by machines."}),Object(h.jsx)("a",{href:"https://arxiv.org/pdf/2107.08573.pdf",target:"_",className:"bckgrnd stretched-link"})]})]}),Object(h.jsxs)(b.a,{className:"project mx-auto",children:[Object(h.jsx)(b.a.Header,{className:"bckgrnd cardbodytext",children:"Recognizing Emotion in the Wild using Multimodal Data"}),Object(h.jsxs)(b.a.Body,{className:"bckgrnd cardbodytext text-left",children:[Object(h.jsx)("p",{children:Object(h.jsx)("b",{children:"Shivam Srivastava, Saandeep Aathreya, Saurabh Hinduja, Sk Rahatul Jannat, Hamza Elhamdadi, and Shaun Canavan"})}),Object(h.jsx)("h6",{children:"Abstract:"}),Object(h.jsx)("p",{children:"In this work, we present our approach for all four tracks of the eighth Emotion Recognition in the Wild Challenge (EmotiW 2020). The four tasks are group emotion recognition, driver gaze prediction, predicting engagement in the wild, and emotion recognition using physiological signals. We explore multiple approaches including classical machine learning tools such as random forests, state of the art deep neural networks, and multiple fusion and ensemblebased approaches. We also show that similar approaches can be used across tracks as many of the features generalize well to the different problems (e.g. facial features). We detail evaluation results that are either comparable to or outperform the baseline results for both the validation and testing for most of the tracks."}),Object(h.jsx)("a",{href:"https://dl.acm.org/doi/pdf/10.1145/3382507.3417970",target:"_",className:"bckgrnd stretched-link"})]})]})]})})})})]})},p=a(34);a(101);var u=function(){return Object(h.jsxs)(h.Fragment,{children:[Object(h.jsx)(d.a,{className:"mx-auto",variant:"dark",sticky:"top",style:{backgroundColor:"#c3aeff"},children:Object(h.jsxs)(o.a,{children:[Object(h.jsx)(n.b,{className:"nav-link page-logo active",to:"/"}),Object(h.jsx)(n.b,{className:"nav-link active",to:"/",children:Object(h.jsx)("h5",{style:{marginTop:10},children:"Home"})}),Object(h.jsx)(n.b,{className:"nav-link",to:"/publications",children:Object(h.jsx)("h5",{style:{marginTop:10},children:"Publications"})}),Object(h.jsx)(n.b,{className:"nav-link",to:"/cv",children:Object(h.jsx)("h5",{style:{marginTop:10},children:"CV"})})]})}),Object(h.jsx)("div",{className:"App",children:Object(h.jsxs)(i.a,{className:"mx-auto",children:[Object(h.jsx)(p.Worker,{workerUrl:"https://unpkg.com/pdfjs-dist@2.6.347/build/pdf.worker.min.js"}),Object(h.jsx)(r.a,{xs:12,lg:8,className:"mx-auto",children:Object(h.jsx)(p.Viewer,{fileUrl:"/resume.pdf",defaultScale:p.SpecialZoomLevel.PageWidth})})]})})]})};a(102);var g=function(){return Object(h.jsxs)(h.Fragment,{children:[Object(h.jsxs)(d.a,{bg:"dark",variant:"dark",children:[Object(h.jsx)(d.a.Brand,{style:{marginLeft:"10%"},href:"#home",children:"Hamza Elhamdadi"}),Object(h.jsxs)(o.a,{className:"mr-auto",children:[Object(h.jsx)(n.b,{class:"nav-link",to:"/",children:"Home"}),Object(h.jsx)(n.b,{class:"nav-link",to:"/Publications",children:"Publications"}),Object(h.jsx)(n.b,{class:"nav-link active",to:"/art",children:"My Art"})]})]}),Object(h.jsx)("div",{className:"App",children:Object(h.jsx)("header",{className:"App-header",children:Object(h.jsx)(i.a,{children:Object(h.jsxs)(l.a,{children:[Object(h.jsx)(r.a,{lg:2}),Object(h.jsx)(r.a,{xs:12,lg:10,children:Object(h.jsx)(l.a,{id:"grid"})})]})})})}),Object(h.jsx)("script",{children:"$('.grid').isotope(specs)"})]})},O=function(e){e&&e instanceof Function&&a.e(3).then(a.bind(null,111)).then((function(t){var a=t.getCLS,s=t.getFID,c=t.getFCP,n=t.getLCP,i=t.getTTFB;a(e),s(e),c(e),n(e),i(e)}))},f=a(7),v=document.getElementById("root");c.a.render(Object(h.jsx)(n.a,{children:Object(h.jsxs)(f.c,{children:[Object(h.jsx)(f.a,{exact:!0,path:"/",component:m}),Object(h.jsx)(f.a,{path:"/publications",component:x}),Object(h.jsx)(f.a,{path:"/art",component:g}),Object(h.jsx)(f.a,{path:"/cv",component:u})]})}),v),O()},27:function(e,t,a){},47:function(e,t){},56:function(e,t,a){},96:function(e,t){},97:function(e,t){},98:function(e,t){},99:function(e,t){}},[[109,1,2]]]);
//# sourceMappingURL=main.761318fc.chunk.js.map