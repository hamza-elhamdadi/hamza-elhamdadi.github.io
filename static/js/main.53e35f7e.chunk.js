(this["webpackJsonphamza-web"]=this["webpackJsonphamza-web"]||[]).push([[0],{100:function(e,t){},101:function(e,t){},102:function(e,t){},103:function(e,t){},104:function(e,t){},113:function(e,t,a){"use strict";a.r(t);a(0);var i=a(57),s=a.n(i),n=(a(65),a(10)),o=a(18),r=a(16),c=a(11),l=a(19),d=a(14),h=a(49),m=(a(34),a(35),a(1));var u=function(){return Object(m.jsxs)(m.Fragment,{children:[Object(m.jsx)(d.a,{className:"mx-auto",variant:"dark",sticky:"top",style:{backgroundColor:"#c3aeff"},children:Object(m.jsxs)(l.a,{children:[Object(m.jsx)(n.b,{className:"nav-link page-logo active",to:"/"}),Object(m.jsx)(n.b,{className:"nav-link active",to:"/",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"Home"})}),Object(m.jsx)(n.b,{className:"nav-link",to:"/publications",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"Publications"})}),Object(m.jsx)(n.b,{className:"nav-link",to:"/cv",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"CV"})})]})}),Object(m.jsx)("div",{className:"App",children:Object(m.jsx)(o.a,{fluid:!0,className:"mx-auto",style:{margin:"1rem 3rem 0 3rem"},children:Object(m.jsxs)(r.a,{className:"mx-auto",children:[Object(m.jsx)(c.a,{xs:0,sm:0,md:0,lg:3,xl:3,xxl:3}),Object(m.jsxs)(c.a,{xs:12,sm:12,md:8,lg:4,xl:3,xxl:3,className:"text-center mx-auto profile-pic",children:[Object(m.jsx)("div",{className:"cropped mx-auto",children:Object(m.jsx)("img",{src:"/IMG_7839_Original.jpg",className:"profile",alt:"Hamza Elhamdadi"})}),Object(m.jsx)("div",{className:"bckgrnd cardbodytext mx-auto",children:Object(m.jsx)("h1",{children:"Hamza Elhamdadi"})})]}),Object(m.jsx)(c.a,{xs:0,sm:0,md:0,lg:3,xl:7,xxl:6}),Object(m.jsx)(c.a,{xs:0,sm:0,md:0,lg:0,xl:3,xxl:3}),Object(m.jsxs)(c.a,{xs:12,sm:12,md:12,lg:12,xl:6,xxl:6,children:[Object(m.jsx)(r.a,{style:{marginTop:"10px",color:"#7050a0"},children:Object(m.jsxs)(c.a,{xs:12,sm:12,md:8,lg:8,xl:12,xxl:12,className:"text-left mx-auto",children:[Object(m.jsx)("h2",{children:"About Me"}),Object(m.jsxs)("p",{style:{marginBottom:10},children:["I am a doctoral student in the ",Object(m.jsx)("a",{style:{margin:0},target:"_",href:"https://groups.cs.umass.edu/hci-vis/",children:" HCI-Vis Lab"}),"\xa0 at the ",Object(m.jsx)("a",{style:{margin:0},target:"_",href:"https://www.umass.edu",children:" University of Massachusetts Amherst"}),". I joined UMass in the Fall of 2021. I received my Master's and Bachelor's degrees from the Computer Science and Engineering at the ",Object(m.jsx)("a",{style:{margin:0},target:"_",href:"https://usf.edu",children:"University of South Florida"}),"."]}),Object(m.jsxs)("p",{style:{marginBottom:"2%"},children:["I am currently pursuing a Ph.D. under the advising of ",Object(m.jsx)("a",{style:{margin:0},target:"_",href:"https://cyxiong.com",children:"Dr. Cindy Xiong"}),". My research interests are at the crossroads of Cognitive Psychology and Computer Science. Specifically, I am interested in how Perception interacts with Data Visualization. My recent published work considers how we can measure trust in Visual Data Communication by implementing existing tools from Social Sciences Research like Trust Games, Perceptual Fluency, and Belief Updating."]})]})}),Object(m.jsx)(r.a,{className:"text-left mx-auto",style:{color:"#7050a0"},children:Object(m.jsxs)(c.a,{className:"bckgrnd cardbodytext news mx-auto",style:{marginBottom:"2%"},xs:12,sm:12,md:8,lg:8,xl:12,children:[Object(m.jsx)("h4",{children:"Recent News"}),Object(m.jsxs)("ul",{className:"table-list",children:[Object(m.jsx)("li",{children:Object(m.jsxs)("p",{children:["Paper accepted to VIS BELIV Workshop 2022     ",Object(m.jsx)("a",{href:"https://arxiv.org/pdf/2209.14276.pdf",target:"_",style:{textAlign:"right"},children:"Click Here to Read the Paper!"})]})}),Object(m.jsx)("li",{children:Object(m.jsxs)("p",{children:["Paper accepted to VIS TREX Workshop 2022      ",Object(m.jsx)("a",{href:"https://arxiv.org/pdf/2209.14340.pdf",target:"_",style:{textAlign:"right"},children:"Click Here to Read the Paper!"})]})}),Object(m.jsx)("li",{children:Object(m.jsxs)("p",{children:["Honorable Mention for Best Poster at VIS 2022 ",Object(m.jsx)("a",{href:"/visposter2022.pdf",target:"_",style:{textAlign:"right"},children:"Click Here to View the Poster!"})]})}),Object(m.jsx)("li",{children:Object(m.jsxs)("p",{children:["Paper accepted to IEEE Vis 2021 and TVCG ",Object(m.jsx)("a",{href:"https://www.youtube.com/watch?v=buqJx0w5PiQ",target:"_",style:{textAlign:"right"},children:"Watch my IEEE Vis Presentation here!"})]})}),Object(m.jsx)("li",{children:Object(m.jsx)("p",{children:"Accepted to Ph.D. program at UMass Amherst - Fall 2021"})})]})]})})]}),Object(m.jsx)(c.a,{xs:0,sm:0,md:0,lg:0,xl:3,xxl:3}),Object(m.jsxs)(c.a,{xs:12,sm:12,md:8,lg:8,xl:3,xxl:3,className:"mx-auto twitter-timeline",children:[Object(m.jsx)(h.b,{dataSource:{sourceType:"profile",screenName:"ElhamdadiHamza"},options:{height:"800vh",chrome:"noheader, nofooter"}}),Object(m.jsx)(h.a,{username:"ElhamdadiHamza",options:{size:"small"}})]})]})})})]})},p=a(33),g=a(58);function j(e){return Object(m.jsxs)(p.a,{className:"project",children:[Object(m.jsx)(p.a.Header,{className:"bckgrnd cardbodytext hdr",children:e.paperName}),Object(m.jsx)(p.a.Img,{src:e.img,className:"teaser",alt:e.altText}),Object(m.jsxs)(p.a.Body,{className:"bckgrnd cardbodytext text-left",children:[Object(m.jsx)("p",{children:Object(m.jsx)("b",{children:e.authors})}),Object(m.jsx)("h6",{children:"Abstract:"}),Object(m.jsx)("p",{children:e.abstract}),Object(m.jsxs)("h6",{children:["Venue: ",e.venue]}),Object(m.jsx)("a",{href:e.link,target:"_",className:"bckgrnd stretched-link"}),Object(m.jsx)("p",{className:"tags ".concat(e.tagType)})]})]},e.key)}var b=function(){return Object(m.jsxs)(m.Fragment,{children:[Object(m.jsx)(d.a,{className:"mx-auto",variant:"dark",sticky:"top",style:{backgroundColor:"#c3aeff"},children:Object(m.jsxs)(l.a,{children:[Object(m.jsx)(n.b,{className:"nav-link page-logo active",to:"/"}),Object(m.jsx)(n.b,{className:"nav-link active",to:"/",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"Home"})}),Object(m.jsx)(n.b,{className:"nav-link",to:"/publications",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"Publications"})}),Object(m.jsx)(n.b,{className:"nav-link",to:"/cv",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"CV"})})]})}),Object(m.jsx)("div",{className:"App",children:Object(m.jsx)(o.a,{className:"mx-auto",style:{color:"#7050a0"},children:Object(m.jsx)(r.a,{children:Object(m.jsx)(c.a,{xs:12,sm:12,md:8,lg:8,xl:8,xxl:8,className:"mx-auto",children:g.map(j)})})})})]})},f=a(43);a(105);var x=function(){return Object(m.jsxs)(m.Fragment,{children:[Object(m.jsx)(d.a,{className:"mx-auto",variant:"dark",sticky:"top",style:{backgroundColor:"#c3aeff"},children:Object(m.jsxs)(l.a,{children:[Object(m.jsx)(n.b,{className:"nav-link page-logo active",to:"/"}),Object(m.jsx)(n.b,{className:"nav-link active",to:"/",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"Home"})}),Object(m.jsx)(n.b,{className:"nav-link",to:"/publications",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"Publications"})}),Object(m.jsx)(n.b,{className:"nav-link",to:"/cv",children:Object(m.jsx)("h5",{style:{marginTop:10},children:"CV"})})]})}),Object(m.jsx)("div",{className:"App",children:Object(m.jsxs)(o.a,{className:"mx-auto",children:[Object(m.jsx)(f.Worker,{workerUrl:"https://unpkg.com/pdfjs-dist@2.6.347/build/pdf.worker.min.js"}),Object(m.jsx)(c.a,{className:"mx-auto",children:Object(m.jsx)(f.Viewer,{fileUrl:"/CV_Hamza_Elhamdadi.pdf",defaultScale:f.SpecialZoomLevel.PageWidth})})]})})]})};a(106);var v=function(){return Object(m.jsxs)(m.Fragment,{children:[Object(m.jsxs)(d.a,{bg:"dark",variant:"dark",children:[Object(m.jsx)(d.a.Brand,{style:{marginLeft:"10%"},href:"#home",children:"Hamza Elhamdadi"}),Object(m.jsxs)(l.a,{className:"mr-auto",children:[Object(m.jsx)(n.b,{class:"nav-link",to:"/",children:"Home"}),Object(m.jsx)(n.b,{class:"nav-link",to:"/Publications",children:"Publications"}),Object(m.jsx)(n.b,{class:"nav-link active",to:"/art",children:"My Art"})]})]}),Object(m.jsx)("div",{className:"App",children:Object(m.jsx)("header",{className:"App-header",children:Object(m.jsx)(o.a,{children:Object(m.jsxs)(r.a,{children:[Object(m.jsx)(c.a,{lg:2}),Object(m.jsx)(c.a,{xs:12,lg:10,children:Object(m.jsx)(r.a,{id:"grid"})})]})})})}),Object(m.jsx)("script",{children:"$('.grid').isotope(specs)"})]})},y=function(e){e&&e instanceof Function&&a.e(3).then(a.bind(null,115)).then((function(t){var a=t.getCLS,i=t.getFID,s=t.getFCP,n=t.getLCP,o=t.getTTFB;a(e),i(e),s(e),n(e),o(e)}))},O=a(3);s.a.createRoot(document.getElementById("root")).render(Object(m.jsx)(n.a,{children:Object(m.jsxs)(O.c,{children:[Object(m.jsx)(O.a,{exact:!0,path:"/",element:Object(m.jsx)(u,{})}),Object(m.jsx)(O.a,{path:"/publications",element:Object(m.jsx)(b,{})}),Object(m.jsx)(O.a,{path:"/art",element:Object(m.jsx)(v,{})}),Object(m.jsx)(O.a,{path:"/cv",element:Object(m.jsx)(x,{})})]})})),y()},35:function(e,t,a){},53:function(e,t){},58:function(e){e.exports=JSON.parse('[{"key":"paper5","paperName":"How Do We Measure Trust in Visual Data Communication?","img":"/measuring_trust_teaser.png","altText":"An icon of a person holding money in the middle left of the image labeled trustor. Two arrows pointing from the trustor to the top right and bottom right of the image. An icon of a person labeled Trustee 1 in the top right. An icon of a person labeled Trustee 2 in the bottom right.","authors":"Hamza Elhamdadi, Aimen Gaba, Yea-Seul Kim, Cindy Xiong","abstract":"Trust is fundamental to effective visual data communication between the visualization designer and the reader. Although personal experience and preference influence readers\u2019 trust in visualizations, visualization designers can leverage design techniques to create visualizations that evoke a \u201ccalibrated trust,\u201d at which readers arrive after critically evaluating the information presented. To systematically un- derstand what drives readers to engage in \u201ccalibrated trust,\u201d we must first equip ourselves with reliable and valid methods for measuring trust. Computer science and data visualization researchers have not yet reached a consensus on a trust definition or metric, which are essential to building a comprehensive trust model in human- data interaction. On the other hand, social scientists and behavioral economists have developed and perfected metrics that can measure generalized and interpersonal trust, which the visualization community can reference, modify, and adapt for our needs. In this paper, we gather existing methods for evaluating trust from other disciplines and discuss how we might use them to measure, define, and model trust in data visualization research. Specifically, we discuss quan- titative surveys from social sciences, trust games from behavioral economics, measuring trust through measuring belief updating, and measuring trust through perceptual methods. We assess the potential issues with these methods and consider how we can systematically apply them to visualization research.","venue":"IEEE BELIV, 2022","link":"https://arxiv.org/pdf/2209.14276.pdf","tagType":"workshop-tag"},{"key":"paper4","paperName":"Using Processing Fluency as a Metric of Trust in Scatterplot Visualizations","img":"/tweet_visual_content.png","altText":"Seven scatterplots in a row. From left to right, the scatterplots are labeled Blur, Opacity, Outline, Gridlines, Scale, Overlap, and Control. Each scatterplot shows a different dataset camouflaged by the type of camouflage in its label.","authors":"Hamza Elhamdadi, Lace Padilla, Cindy Xiong","abstract":"Establishing trust with readers is an important first step in visual data communication. But what makes a visualization trustworthy? Psychology and behavioral economics research has found processing fluency (i.e., speed and accuracy of perceiving and processing a stimulus) is central to perceived trust. We examine the association between processing fluency and trust in visualizations through two empirical studies. In Experiment 1, we tested the effect of camouflaging a visualization on processing fluency. Participants estimated the proportion of data values within a specified range for six camouflaged visualizations and one non-camouflaged control; they also reported their perceived difficulty for each of the visualizations. Camouflaged visualizations produced less accurate estimations compared to the control. In Experiment 2, we created a decision task based on trust games adapted from behavioral economics. We asked participants to invest money in two hypothetical companies and report how much they trust each company. One company communicates its strategy with a camouflaged visualization, the other with a controlled visualization. Participants tended to invest less money in the company presenting a camouflaged visualization. Hence, we found support for the hypothesis that processing fluency is key to the perception of trust in visual data communication.","venue":"IEEE TREX, 2022","link":"https://arxiv.org/pdf/2209.14340.pdf","tagType":"workshop-tag"},{"key":"paper3","paperName":"Processing Fluency Improves Trust In Scatterplot Visualizations","img":"/poster_teaser.png","altText":"Top left is an example of visualizations shown to participants of the experiment along with the trust game question. Top right is the same stimulus as the top right but the question is now a subjective trust rating. The bottom of the image is a boxplot for each question showing the distribution of participants answers for the camouflage and control conditions.","authors":"Hamza Elhamdadi, Lace Padilla, Cindy Xiong","abstract":"Trust plays a significant role in how people perceive, discount, or dismiss scientific information and make critical decisions. Therefore, establishing trust is a critical first step in visual data communication. But what makes a visualization trustworthy? Researchers in psychology and behavioral economics have identified processing fluency (i.e., speed and accuracy of perceiving and processing a stimulus) as a key factor impacting trust perception. We examine the association between processing fluency and trust in visualizations through two empirical studies. We manipulated fluency by creating camouflaged visualizations that are more difficult to process. In Experiment 1, we tested the effect of camouflaging a visualization on processing fluency. Participants completed a perceptual task with six camouflaged visualizations and one non-camouflaged control. The task involved estimating the proportion of data values within a range and reporting the difficulty of doing so. We found the camouflaged visualizations produced less accurate estimations compared to the control. In Experiment 2, we created a decision task based on trust games adapted from behavioral economics. We asked participants to invest money in two hypothetical companies and report how much they trust each company. One company communicates its strategy with a camouflaged visualization, the other with a controlled visualization. The results revealed that participants tend to invest less money in the company presenting a camouflaged visualization. We found that processing fluency is a key factor in the perception of trust in visual data communication.","venue":"IEEE VIS Posters, 2022","link":"/visposter2022.pdf","tagType":"poster-tag"},{"key":"paper2","paperName":"AffectiveTDA: Using Topological Data Analysis to Improve Analysis and Explainability in Affective Computing","img":"/affectiveTDA_teaser.png","altText":"Top of the image shows the creation of a persistence diagram by forming simplexes on 2D facial landmarks. The bottom of the image shows output of the topology pipeline for using TSNE and UMAP dimension reduction.","authors":"Hamza Elhamdadi, Shaun Canavan, Paul Rosen","abstract":"We present an approach utilizing Topological Data Analysis to study the structure of face poses used in affective computing, i.e., the process of recognizing human emotion. The approach uses a conditional comparison of different emotions, both respective and irrespective of time, with multiple topological distance metrics, dimension reduction techniques, and face subsections (e.g., eyes, nose, mouth, etc.). The results confirm that our topology-based approach captures known patterns, distinctions between emotions, and distinctions between individuals, which is an important step towards more robust and explainable emotion recognition by machines.","venue":"IEEE VIS, 2021","link":"https://arxiv.org/pdf/2107.08573.pdf","tagType":"conference-tag"},{"key":"paper1","paperName":"Recognizing Emotion in the Wild using Multimodal Data","img":"/multimodal_teaser.png","altText":"Diagram of the Group emotion recognition architecture. The input video is separated into visual and audio features. The visual and audio features are passed through separate Xception networks. The output of the two Xception networks is fused and sent through a fully-connected layer which outputs the class and the negative of the input video.","authors":"Shivam Srivastava, Saandeep Aathreya, Saurabh Hinduja, Sk Rahatul Jannat, Hamza Elhamdadi, and Shaun Canavan","abstract":"In this work, we present our approach for all four tracks of the eighth Emotion Recognition in the Wild Challenge (EmotiW 2020). The four tasks are group emotion recognition, driver gaze prediction, predicting engagement in the wild, and emotion recognition using physiological signals. We explore multiple approaches including classical machine learning tools such as random forests, state of the art deep neural networks, and multiple fusion and ensemblebased approaches. We also show that similar approaches can be used across tracks as many of the features generalize well to the different problems (e.g. facial features). We detail evaluation results that are either comparable to or outperform the baseline results for both the validation and testing for most of the tracks.","venue":"ICMI, 2020","link":"https://dl.acm.org/doi/pdf/10.1145/3382507.3417970","tagType":"conference-tag"}]')},65:function(e,t,a){}},[[113,1,2]]]);
//# sourceMappingURL=main.53e35f7e.chunk.js.map